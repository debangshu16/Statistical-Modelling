---
title: "Assignment 2"
author: "Debangshu Bhattacharya"
date: "03/10/2020"
output: html_document
---

```{r setup include = False}
knitr::opts_chunk$set(echo = TRUE)
```

**Dataset used**
The data I have used is the dataset "UKNonDurables" which is publicly available. The dataset consists of quarterly consumption of Non Durables in UK from 1st quarter of 1955 to 4th quarter of 1988. The dataset consists of 136 total observations.
The documentation is found in the link https://vincentarelbundock.github.io/Rdatasets/doc/AER/UKNonDurables.html.


```{r include = False}
library(tseries)
library(forecast)
library(dplyr)
library(moments)
```


```{r}
#Reading the data
data = read.table('UKNonDurables.csv',sep=',',header=T,stringsAsFactors = F)
head(data)
```

**Step 1: Preparing the dataset**

```{r}
#Only keeping two columns : Time and Value
data = data[, c('time','value')]
head(data)

#Ordering data based on time
data = data[order(data$time),]

#Check for missing data
paste("Number of Missing values is", sum(is.na(data)))
#So there are no missing values in the dataset

#Cleaning data
start_date = data$time[1]
value = ts(data$value, start = start_date,freq =4)
value = tsclean(value)
data$value = value

#Dividing data into train and test sets where test set contains the last 6 observations
tm = data$time

period<- 6
n = length(value)
m = n - period


#Split data into train data and test data with test data having last 6 observations

train_lim = length(data$value) - period
data['log_value'] = log(data$value)

train_data = data[1:train_lim,]
test_data = data[(train_lim+1):length(data$value),]

```

**Step 2 : Plot of the total data **
```{r}
#Time series plot of the whole data
ts.plot(value,main = "Consumption of Non Durables in UK")
```

**Step 3: Descriptive statistics of the data  **
```{r}
value = ts(train_data$value, start = start_date, frequency = 4)
descriptive_stats = list('Mean' = mean(value),'Median' = median(value),'Minimum' = min(value),
                         'Maximum' = max(value), 'Standard Deviation' = sd(value) )
descriptive_stats$skewness = skewness(value)
descriptive_stats$kurtosis = kurtosis(value)

f <-function(x,mean1,sd1)
{
  #print (paste (mean1,sd1))
  count = 0
  for (i in c(1:length(x)))
  {
    if ((x[i]>=(mean1-3*sd1)) && (x[i]<=(mean1+3*sd1)))
      count = count+1
  }
  
  return ((count*100)/(length(x)))
}
descriptive_stats$perct_within_3sd = f(value,descriptive_stats$Mean,descriptive_stats$`Standard Deviation`)

str(descriptive_stats)


```
So, the descriptive statistics of the train dataset can be seen from above. Note, that the data has a wide span of values and a high standard deviation of 7534.

**Step 4: Testing for stationarity/Coming up with Transformations to make it stationary**
First, let's check for the stationarity of the data using the three tests(ADF, KPSS and PP).
```{r}
adf.test(value)
```
```{r}
kpss.test(value)
```
```{r}
pp.test(value)
```
Although, the PP test says that the data is stationary with a 5% level of significance, we cannot say that the data is stationary as the other two tests reject stationarity.

**Step 5: Coming up with the transformation of data**
We have already seen that the values of the data span a wide range. So, a log transformation may be a good idea. Anyways we plot the time series of first difference of the data as follows:
```{r}
ts.plot(diff(value),main = "First Difference time series plot")
```
We see that the variance does not seem constant. We also had seen that the range of the data is very large.So, we take the log transform of the data and proceed.
```{r}
value = log(value)

adf = adf.test(value)
print (adf)
kpss = kpss.test(value)
print (kpss)
pp = pp.test(value)
print (pp)
```
The log of the data is not stationary but let's see the first difference of the log of the values.
```{r}
ts.plot(diff(value),main = "Difference of logs of Consumption of Non Durables in UK")
```
This look's much better than the first difference of the values. Let's do the stationarity tests on these.
```{r}
adf = adf.test(diff(value))
print (adf)
kpss = kpss.test(diff(value))
print (kpss)
pp = pp.test(diff(value))
print (pp)
```
As can be seen from the tests, the first difference of the log transform of the data is stationary. So, we get the difference order as 1. We also confirm this with the function "ndiffs" which give the difference order to make it stationary for a given test(KPSS by default).

**Step 6: Fitting ARIMA models**
a) AIC model
```{r}
#Fitting model which has minimum AIC
d = 1
model_fit = auto.arima(value,seasonal = F,d=d,max.p = 5, max.q = 5, ic = "aic")
model_fit
```
b)BIC model
```{r}
model_fit = auto.arima(value,seasonal = F,d=d,max.p = 5, max.q = 5, ic = "bic")
model_fit
```
So, the same model is selected for both AIC and BIC. We are going to proceed with this model.

**Step 7: Forecasting and evaluating the model**
```{r}
plot(forecast(model_fit,h=period),type='l')
lines(test_data$time, test_data$log_value )
```
As can be seen from the graph above, the blue line is the forecasted value of the log of the consumption and the black line is the log of the actual value of consumption. The model seems to capture the trend well. However, we have to take the exponential of the forecasted value to get the actual forecast.

We do that conversion and print the mse as follows:
```{r}
forecasted_values = forecast(model_fit,h = period)
forecasted_values = as.numeric(forecasted_values$mean)

forecasted_values = lapply(forecasted_values,exp)
forecasted_values = as.numeric(forecasted_values)

true_values = data$value[(m+1):(m+period)]

mse = mean((true_values-forecasted_values)^2)

mse
```
The value looks quite big. But as we saw from the descriptive stats of the data, the training data itself spans over a wide range and has a variance of 56761482 as shown below:
```{r}
descriptive_stats$`Standard Deviation`^2
```
So, this may be a reason for such huge mse value as the model otherwise seems to capture the trend pretty well as can be shown in the graph below:
```{r}
plot(NULL,xlim = c(tm[1],tm[n]),ylim = c(20000,70000),ylab = 'Non Durables consumption',xlab='Time')
points(data$time, data$value,type='l',lwd=2,col='blue')
points(test_data$time, test_data$value,type='l',lwd=2,col='black')
points(test_data$time, forecasted_values,type='l',lwd=2,col='green')
legend("bottomright",
       legend =  c("train_values","test_values","forecasted_values"),
       col = c("blue","black","green"),lty = 1,
       cex = 0.8)

abline(v=tm[m+1],col='grey')
```
The comparison of true value and forecasted value is shown below:
```{r}
data.frame("True_value"=true_values, "Forecasted_values" = forecasted_values)
```
The model seems to be pretty close to the true values initially but fails to capture the increased spike in the last two quarters leading to a high mse.